{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "import os \n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"font.size\"] = 15\n",
    "#import seaborn as sns\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", 101)\n",
    "import glob\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "\n",
    "input_dir = \"./fashion/input/\"\n",
    "'''\n",
    "One Dataset is around more than 200mb\n",
    "The size of one batch is mainly determined by batch size, image size, maximum object number\n",
    "batch size = IMAGES_PER_GPU*GPU_COUNT in config.py\n",
    "image size is related to MAX_IMG_DIM,MIN_IMG_DIM in config.py\n",
    "maximum object numbers = MAX_GT_OBJECTS in config.py\n",
    "suppose dtpye is int8 --> at least batch_size*1024*1024*2*100 = batch_size*200mb\n",
    "\n",
    "This library use keras' fit_generator with argument multiprocessing=true, workers=cpu_count\n",
    "This means multiple dataset generators are instanciated by each cpu\n",
    "each cpu=process keeps queue for pipelining, and we can limit the maxmum number of elements in queue by setting max_queue_size in fit_generator function\n",
    "*it doesnâ€™t mean the training begins only after the queue is filled. Making the yield super-slow shows this.\n",
    "--> However, if training(consumer) is slower than generating data, queue gets filled, resulting memory blowing up\n",
    "**Trade-off: if batch size increases, it can help optimize better(gradienet is more accurate), but it can slow dowin the one training step and make queue filled. On the other hand, if batch size is small, whole training time should be longer due to approximate gradient, but one training step is faster, so it does not make queu filled as quickly as when batch size is bigger.\n",
    "***multiprocessing=false,workers>1 or workers>cpu count-->need for thread-safe generator\n",
    "****This library workers=cpu count, so you don't need to worry about thread safety\n",
    "Anyway, avoiding 'out of memory' you need to change some parameters, and The arguments that I think is better to change is : max_queue_size(defualt 100), workers(less than cpu count), IMAGE_PER_GPU(default 1), GPU_COUNT\n",
    "\n",
    "Example\n",
    "IMAGE_PER_GPU=8/CPU=16/max_queue_size=100/GPU_COUNT=1\n",
    "--> maximum size is at least 16*8*1*100*data ==> 2500G!!!!!\n",
    "120/\n",
    "\n",
    "\n",
    "KAGGLE KERNEL\n",
    "CPU=4/RAM=17G --> CPU specifications\n",
    "CPU=2/RAM=14G --> GPU specifications\n",
    "IMAGE_PER_GPU=1/max_queu_size=100\n",
    "--> maximum size is at least 2*1*1*100*data ==> 40G TT\n",
    "\n",
    "NASH: CPU 32G/ cores: 8 \n",
    "--> maximum 8*batch_size*max_queue_size*data-->2G*batch_size*max_queue_size\n",
    "20/() -> 10 >= batch_size*max_queue_size super safe\n",
    "\n",
    "'''\n",
    "\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2,3\"\n",
    "\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "config.allow_soft_placement=True\n",
    "config.log_device_placement=False\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "\n",
    "\n",
    "\n",
    "def classid2label(class_id):\n",
    "    category, *attribute = class_id.split(\"_\")\n",
    "    return category, attribute\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def json2df(data):\n",
    "    df = pd.DataFrame()\n",
    "    for index, el in enumerate(data):\n",
    "        for key, val in el.items():\n",
    "            df.loc[index, key] = val\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "input_dir = \"advanced_ML/data/\"\n",
    "train_df = pd.read_csv(input_dir + \"train.csv\")\n",
    "train_df.head()\n",
    "\n",
    "\n",
    "\n",
    "with open(input_dir + \"label_descriptions.json\") as f:\n",
    "    label_description = json.load(f)\n",
    "    \n",
    "print(\"this dataset info\")\n",
    "print(json.dumps(label_description[\"info\"], indent=2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "category_df = json2df(label_description[\"categories\"])\n",
    "category_df[\"id\"] = category_df[\"id\"].astype(int)\n",
    "category_df[\"level\"] = category_df[\"level\"].astype(int)\n",
    "attribute_df = json2df(label_description[\"attributes\"])\n",
    "attribute_df[\"id\"] = attribute_df[\"id\"].astype(int)\n",
    "attribute_df[\"level\"] = attribute_df[\"level\"].astype(int)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "print(\"Category Labels\")\n",
    "#category_df\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "print(\"Attribute Labels\")\n",
    "#attribute_df\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#sample_rle_encoding=train_df['EncodedPixels'][0]\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "counter_category = Counter()\n",
    "counter_attribute = Counter()\n",
    "for class_id in train_df[\"ClassId\"]:\n",
    "    category, attribute = classid2label(class_id)\n",
    "    counter_category.update([category])\n",
    "    counter_attribute.update(attribute)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "category_name_dict = {}\n",
    "for i in label_description[\"categories\"]:\n",
    "    category_name_dict[str(i[\"id\"])] = i[\"name\"]\n",
    "attribute_name_dict = {}\n",
    "for i in label_description[\"attributes\"]:\n",
    "    attribute_name_dict[str(i[\"id\"])] = i[\"name\"]\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "ROOT_DIR=\"fashion/mrcnn\"\n",
    "DATA_DIR=\"advanced_ML/data\"\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "sys.path.append(os.path.join(ROOT_DIR, 'Mask_RCNN'))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "class FashionDataset(utils.Dataset):\n",
    "    def load_fashion(self,image_ids=None,num_data=None):\n",
    "        '''\n",
    "        add_class --> register 46 classes self.add_class('fashion',i,name)\n",
    "        image_ids --> unique index of images(name?)\n",
    "        self.add_image('fashion',image_ids,width,height,annotations)\n",
    "        width,height --> shape[:2] or extract form dataframe\n",
    "        annotations --> all collections of annotations for each image\n",
    "        Todo:\n",
    "        There are some rows that have height and weight as nan value\n",
    "        validation option is necessary for training\n",
    "        '''\n",
    "        for i,row in category_df.iterrows():\n",
    "            self.add_class('fashion',i,row['name'])\n",
    "        \n",
    "        if image_ids is None:\n",
    "            image_ids = list(set(train_df['ImageId']))\n",
    "        \n",
    "        if num_data is not None:\n",
    "            random.seed(42)\n",
    "            random.shuffle(image_ids)\n",
    "            image_ids=image_ids[:num_data]\n",
    "            \n",
    "            \n",
    "        for i in image_ids:\n",
    "            Width = train_df[train_df['ImageId']==i]['Width'].reset_index(drop=True)[0]\n",
    "            Height = train_df[train_df['ImageId']==i]['Height'].reset_index(drop=True)[0]\n",
    "            self.add_image('fashion',\n",
    "                           image_id=i,\n",
    "                           path=DATA_DIR+'/train/'+i,\n",
    "                           width=Width,\n",
    "                           height=Height,\n",
    "                           annotations=train_df[train_df['ImageId']==i])\n",
    "        \n",
    "        \n",
    "    def image_reference(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "        return info['path']\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "        ImagePath = info['path']\n",
    "        image = np.asarray(Image.open(ImagePath).convert(\"RGB\"))\n",
    "        return image\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "        annotations = info['annotations']\n",
    "        width=info['width']\n",
    "        height=info['height']\n",
    "        \n",
    "        instance_masks = []\n",
    "        class_ids = []\n",
    "        \n",
    "        for i,annotation in annotations.iterrows():\n",
    "            class_id=annotation['ClassId']\n",
    "            class_id=class_id.split('_')[0]\n",
    "            class_ids.append(class_id)\n",
    "            rle = annotation['EncodedPixels']\n",
    "            instance_masks.append(self.rle_to_mask(rle,width,height))\n",
    "            \n",
    "        if class_ids:\n",
    "            mask = np.stack(instance_masks, axis=2).astype(np.bool)\n",
    "            class_ids = np.array(class_ids, dtype=np.int32)+1\n",
    "            return mask, class_ids\n",
    "        else:\n",
    "            # Call super class to return an empty mask\n",
    "            return super(FashionDataset, self).load_mask(image_id)\n",
    "        \n",
    "        \n",
    "    def rle_to_mask(self,rle,width,height):\n",
    "        mask = np.zeros(width*height,dtype=np.int8)\n",
    "        pixels_list = list(map(int,rle.split(\" \")))\n",
    "        for i in range(0,len(pixels_list),2):\n",
    "            start_pixel = pixels_list[i]-1\n",
    "            num_pixel = pixels_list[i+1]-1\n",
    "            mask[start_pixel:start_pixel+num_pixel] = 1\n",
    "        \n",
    "        mask = mask.reshape((height,width),order='F')\n",
    "        return mask\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "image_ids_list = list(set(train_df['ImageId']))\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(image_ids_list)\n",
    "\n",
    "val_split = 0.1\n",
    "split = int((1-val_split)*len(image_ids_list))\n",
    "\n",
    "train_ids = image_ids_list[:split]\n",
    "val_ids = image_ids_list[split:]\n",
    "train_ids = train_ids[:10]\n",
    "val_ids = val_ids[:10]\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#fashion_dataset = FashionDataset()\n",
    "#fashion_dataset.load_fashion(num_data=100)\n",
    "#fashion_dataset.prepare()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "fashion_dataset_train = FashionDataset()\n",
    "fashion_dataset_train.load_fashion(train_ids)\n",
    "fashion_dataset_val = FashionDataset()\n",
    "fashion_dataset_val.load_fashion(val_ids)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "fashion_dataset_train.prepare()\n",
    "fashion_dataset_val.prepare()\n",
    "\n",
    "print(\"dataset prepared\")\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#print(\"Image Count: {}\".format(len(fashion_dataset.image_ids)))\n",
    "#print(\"Class Count: {}\".format(fashion_dataset.num_classes))\n",
    "#for i, info in enumerate(fashion_dataset.class_info):\n",
    "#    print(\"{:3}. {:50}\".format(i, info['name']))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Load and display random samples\n",
    "#image_ids = np.random.choice(fashion_dataset.image_ids, 4)\n",
    "#for image_id in image_ids:\n",
    "#    image = fashion_dataset.load_image(image_id)\n",
    "#    mask, class_ids = fashion_dataset.load_mask(image_id)\n",
    "#    visualize.display_top_masks(image, mask, class_ids, fashion_dataset.class_names)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Load random image and mask.\n",
    "#image_id = random.choice(fashion_dataset.image_ids)\n",
    "#image = fashion_dataset.load_image(image_id)\n",
    "#mask, class_ids = fashion_dataset.load_mask(image_id)\n",
    "# Compute Bounding box\n",
    "#bbox = utils.extract_bboxes(mask)\n",
    "\n",
    "# Display image and additional stats\n",
    "#print(\"image_id \", image_id, fashion_dataset.image_reference(image_id))\n",
    "#log(\"image\", image)\n",
    "#log(\"mask\", mask)\n",
    "#log(\"class_ids\", class_ids)\n",
    "#log(\"bbox\", bbox)\n",
    "# Display image and instances\n",
    "#visualize.display_instances(image, bbox, mask, class_ids, fashion_dataset.class_names)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Run-length encoding stolen from https://www.kaggle.com/rakhlin/fast-run-length-encoding-python\n",
    "def rle_encoding(x):\n",
    "    dots = np.where(x.T.flatten() == 1)[0]\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if (b>prev+1): run_lengths.extend((b + 1, 1))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    return run_lengths\n",
    "\n",
    "'''\n",
    "def rle_to_mask(rle,width,height):\n",
    "    mask = np.zeros(width*height,dtype=np.int8)\n",
    "    pixels_list = list(map(int,rle.split(\" \")))\n",
    "    for i in range(0,len(pixels_list),2):\n",
    "        start_pixel = pixels_list[i]-1\n",
    "        num_pixel = pixels_list[i+1]-1\n",
    "        mask[start_pixel:start_pixel+num_pixel] = 1\n",
    "        \n",
    "    mask = mask.reshape((height,width),order='F')\n",
    "    return mask\n",
    "\n",
    "\n",
    "\n",
    "m=rle_to_mask(sample_rle_encoding,3676,5214)\n",
    "r=rle_encoding(m)\n",
    "\n",
    "\" \".join([str(e) for e in r]) == sample_rle_encoding\n",
    "'''\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "class FashionConfig(Config):\n",
    "\n",
    "    NAME = \"fashion\"\n",
    "    #IMAGE_MAX_DIM = 512\n",
    "    #IMAGE_MIN_DIM =256\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "    # Uncomment to train on 8 GPUs (default is 1)\n",
    "    GPU_COUNT = 4\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 46\n",
    "    STEPS_PER_EPOCH=1000\n",
    "    \n",
    "config = FashionConfig()\n",
    "#config.display()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                                  model_dir=ROOT_DIR)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#model.log_dir\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "'''\n",
    "if args.model.lower() == \"last\":\n",
    "        # Find last trained weights\n",
    "        model_path = model.find_last()\n",
    "elif args.model.lower() == \"imagenet\":\n",
    "        # Start from ImageNet trained weights\n",
    "        model_path = model.get_imagenet_weights()\n",
    "else:\n",
    "        model_path = args.model\n",
    "'''\n",
    "\n",
    "'''\n",
    "WEIGHT_PATH = 'last'\n",
    "\n",
    "if WEIGHT_PATH == \"last\":\n",
    "        # Find last trained weights\n",
    "        model_path = model.find_last()\n",
    "elif WEIGHT_PATH == \"imagenet\":\n",
    "        # Start from ImageNet trained weights\n",
    "        model_path = model.get_imagenet_weights()\n",
    "else:\n",
    "        pass\n",
    "        #model_path = args.model\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#model.load_weights('fashion/mrcnn/fashion20190608T2206/mask_rcnn_fashion_0044.h5', by_name=True)\n",
    "model.load_weights(model_path, by_name=True)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model_path)\n",
    "\n",
    "epochs_stage1_1=35\n",
    "epochs_stage1_2=40\n",
    "epochs_stage2_1=45\n",
    "epochs_stage2_2=50\n",
    "epochs_stage3_1=55\n",
    "epochs_stage3_2=60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(fashion_dataset_train, fashion_dataset_val,\n",
    "                learning_rate=config.LEARNING_RATE,\n",
    "                epochs=epochs_stage1_1,\n",
    "                layers='heads')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.train(fashion_dataset_train, fashion_dataset_val,\n",
    "                learning_rate=config.LEARNING_RATE,\n",
    "                epochs=epochs_stage1_2,\n",
    "                layers='heads')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training - Stage 2\n",
    "# Finetune layers from ResNet stage 4 and up\n",
    "print(\"Fine tune Resnet stage 4 and up\")\n",
    "model.train(fashion_dataset_train, fashion_dataset_val,\n",
    "            learning_rate=config.LEARNING_RATE,\n",
    "            epochs=epochs_stage2_1,\n",
    "            layers='4+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.train(fashion_dataset_train, fashion_dataset_val,\n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=epochs_stage2_2,\n",
    "            layers='4+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training - Stage 3\n",
    "# Fine tune all layers\n",
    "print(\"Fine tune all layers\")\n",
    "model.train(fashion_dataset_train, fashion_dataset_val,\n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=epochs_stage3_1,\n",
    "            layers='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(fashion_dataset_train, fashion_dataset_val,\n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=epochs_stage3_2,\n",
    "            layers='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(fashion_dataset_train, fashion_dataset_val,\n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=75,layers='4+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(fashion_dataset_train, fashion_dataset_val,\n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=90,\n",
    "            layers='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Finished\")#cyclic learning rate try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
